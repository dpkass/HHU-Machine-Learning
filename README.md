# Machine Learning 2 â€“ Exercise Solutions

**Author:** Taha El Amine Kassabi  
**Course:** Machine Learning 2 (WS 2023/24)  
**Instructor:** Prof. Dr. Dominik Heider  
**University:** Heinrich Heine University DÃ¼sseldorf (HHU)

---

## ğŸ“š Overview

This repository contains solutions to all 11 exercises from the Machine Learning 2 course at HHU. Each assignment covers
foundational or advanced topics in classical machine learning â€” from matrix manipulation and statistics to support
vector machines and neural networks. Most solutions consist of Jupyter notebooks, supplementary markdown or PDF answers,
and linked datasets.

---

## ğŸ“‚ Repository Structure

```
Sheets/
â”œâ”€â”€ Sheet 0 â€“ Conda, YAML, file IO
â”œâ”€â”€ Sheet 1 â€“ Text processing & matrices
â”œâ”€â”€ Sheet 2 â€“ Descriptive statistics, correlation
â”œâ”€â”€ Sheet 3 â€“ PCA & MDS (PCoA)
â”œâ”€â”€ Sheet 4 â€“ Clustering (K-Means, Hierarchical)
â”œâ”€â”€ Sheet 5 â€“ Linear and logistic regression
â”œâ”€â”€ Sheet 6 â€“ k-Nearest Neighbors
â”œâ”€â”€ Sheet 7 â€“ Support Vector Machines
â”œâ”€â”€ Sheet 8 â€“ Decision Trees
â”œâ”€â”€ Sheet 9 â€“ Random Forest & Performance Measures
â””â”€â”€ Sheet 10 â€“ Neural Nets, Backprop, CNN filters
```

Each folder includes a task sheet (`exercise_N.pdf`) and data or helper files. Solutions are
under `Solutions/Exercise N`.

---

## ğŸ§  Topics Covered

| Exercise | Topics                                                                         |
|----------|--------------------------------------------------------------------------------|
| 0        | Conda setup, YAML, reading/writing files                                       |
| 1        | Text parsing, matrix algebra                                                   |
| 2        | Descriptive stats, skewness, correlation                                       |
| 3        | PCA & classical MDS                                                            |
| 4        | K-means, silhouettes, hierarchical clustering                                  |
| 5        | Linear regression, logistic regression                                         |
| 6        | k-Nearest Neighbors (k-NN), normalization                                      |
| 7        | SVM, Lagrange multipliers, separating hyperplanes                              |
| 8        | Decision trees, information gain, Gini                                         |
| 9        | Random forests, Gini importance, AUC, precision/recall                         |
| 10       | McCulloch/Pitts networks, backpropagation, CNN filters, MNIST training (bonus) |

---

## ğŸ’¾ Setup

```bash
pip install -r requirements.txt
```

You may also use JupyterLab or Google Colab for individual notebooks.

---

## ğŸš€ Usage

Navigate to any exercise in `Solutions/Exercise N`, open the notebook(s), and run the cells. Some tasks also
include `.md`, `.pdf`, or `.docx` explanations for handwritten answers.

---

## ğŸ“ Notes

- Most exercises were completed using Python with **NumPy**, **Pandas**, **Matplotlib**, **Scikit-learn**, and **SciPy
  **.
- For neural networks (Exercise 10 bonus), models were implemented using **Keras/TensorFlow** and **PyTorch**.
